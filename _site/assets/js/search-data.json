{"0": {
    "doc": "Overview",
    "title": "Analysis pivots around the data",
    "content": "The process starts with a discussion that helps me understand what you desire to accomplish conceptually through an analytics or machine learning application. We then discuss your data assets in relation to your objectives. The data assets form the pivot around which idea refinement will be done. Your data assets can reside in a few different data sources. Ideally, it would be best if you could get your data team to consolidate the data assets we identified in our discussions into a single source file. Generally, a single use case should address a small reasonable goal, so I would expect to not deal with more than a few, say five, data sources. If you have two or three data sources, I don't mind taking on the consolidation task. The consolidated version forms the first baseline of the data for the rinse and repeat process. The sources and integration of the sources in subsequent iterations should be similar. An illustration of the data snapshots for the process is shown. Flow of Data Snapshots in rinse and repeat ",
    "url": "/about/#analysis-pivots-around-the-data",
    
    "relUrl": "/about/#analysis-pivots-around-the-data"
  },"1": {
    "doc": "Overview",
    "title": "Implementation pivots around your use case",
    "content": "Once we have decided on a set of candidate data assets for your use case, I would need to assess the data quality of the data sources and identify modeling approaches that make sense in light of the discussions we have had up to this point. I would plan a set of activities that should lead to some initial previews of the modeling approaches. These model development approaches are proof of concept sketches on a small sample of the data and are meant to convey and develop an understanding of what a modeling approach can deliver. I will discuss these results with you. If the initial previews are encouraging and we agree on refining this further, I would refine the data preprocessing activities and other supporting analyses. The degree of detail and precision that we capture in our discussion has a bearing on the modeling results. Of course, it is very much possible that you have an epiphany about your use case after we have these discussions and have moved on to the next step, modeling. If this happens and you want to incorporate this into your first iteration, we can come back to this step to reevaluate modeling requirements in light of your new insight. Implementation increments pivot around use case discussions ",
    "url": "/about/#implementation-pivots-around-your-use-case",
    
    "relUrl": "/about/#implementation-pivots-around-your-use-case"
  },"2": {
    "doc": "Overview",
    "title": "Modeling",
    "content": "When we agree on refining and getting a detailed evaluation of the solutions relevant to your use case, I can begin the modeling phase. The details of this phase really depend on whether you have an analytic use case or a machine learning use case. An analytic use case typically performs a computation on all of the data. Computation on all of the data can require the use of a cluster. Sometimes, computation on all of the data is not possible and will require the use of approximation techniques . Typically, there is no learning component in an analytic use case. If you have a machine learning use case, the style of learning you want to adopt, offline or batch machine learning, is an important decision. When to redeploy a model and the kind of learning style that you want to adopt is a decision that is driven by business conditions, your budget, and lead times in your organization for model deployment. If this is your first deployment, an offline model is a reasonable model to begin with. ",
    "url": "/about/#modeling",
    
    "relUrl": "/about/#modeling"
  },"3": {
    "doc": "Overview",
    "title": "A Resonable Processing Template",
    "content": "A template for implementation based on the above steps is as follows. In what follows, the components of an implementation framework are discussed. A reference implementation for an analytics project and a machine learning project is provided. These projects are also recipes . A discussion of a recipe is provided in the next section. The first step of the reference implementation is the select step. This step selects the subset of data that you provide to me that is relevant to the task1. The second step is the denoising step. For each attribute that is relevant, there is a set of admissible values for that attribute. What is admissible depends on the use case and the attribute. Admissible values of the attribute are what can be processed in the modeling or analytics computation. The set of inadmissible values for the attribute is called noise. How noise is processed, called, denoising depends on the use-case and the type of inadmissibility. Missing values can be one type of inadmissibility. Denoising produces a dataset that is ready for further processing. Once the data is denoised, each attribute in the resulting dataset has values that the model building or the analytics computation can process. Data Quality tools can be used to abstract away the notion of noise and attest to the quality of the dataset for modeling and analytics computation2. In order to perform the required modeling or analytics computation, we may need new attributes derived from the attributes in the raw dataset, or we may need to transform the raw attributes to a new form based on some preprocessing recipe. This happens in the transform step. At the end of the transform step, we have a dataset that is ready for modeling. What is done in the modeling step depends on the specifics of the use case. See the provided recipes for a sense of what this step can entail. A template for implementation ",
    "url": "/about/#a-resonable-processing-template",
    
    "relUrl": "/about/#a-resonable-processing-template"
  },"4": {
    "doc": "Overview",
    "title": "Data Products",
    "content": "If you are working with me on a machine learning project, then it is possible that this use generates representations of your dataset that can be consumed by other applications in your organization. Capturing these representations or embeddings as a full-featured data product can be done. There is no recipe for this at the moment, but this should published as a recipe soon3. ",
    "url": "/about/#data-products",
    
    "relUrl": "/about/#data-products"
  },"5": {
    "doc": "Overview",
    "title": "Recipes and Algorithms",
    "content": "A recipe (for the purposes of this documentation) is a data science task that occurs routinely in organizations. The specification of a task is in terms of an organizational or a business context as opposed to a computational description. A robust, satisfactory implementation of this task can be applied with minor changes in a range of applications. A recipe must include the following: . Task Description The task description describes, with suitable examples, the occurrences of the task in organizations and why organizations want to perform the task. Task Data Description The task data description describes the input data format to the task. Task Solution Description The task solution description describes the computational steps involved in developing a solution for the task. An algorithm (for the purposes of this description) has an analogous definition to a recipe except that the specification of an algorithm is in terms of a computational context. It is meant to abstract the details of computation as opposed to a task that is either a part of a use case, or, a use case by itself. The details of the specification are analogous to a task specification. Please read the recipes section of this documentation to see how the above idea is implemented for an analytic recipe and for a machine-learning algorithm. A modeling solution I discuss with you would be described in terms of recipes and algorithms. The discussion of your problem with you will help me understand what recipes and algorithms are a good fit for your use case. ",
    "url": "/about/#recipes-and-algorithms",
    
    "relUrl": "/about/#recipes-and-algorithms"
  },"6": {
    "doc": "Overview",
    "title": "Operationalizing the Solution",
    "content": "When we agree the solution is at a state that you want to operationalize into your business operating environment, I can support you as follows: . | If your use case is a simple report, a small set of APIs that are stand-alone, I can do that for you. For example, if we can use github codespaces, then that would make operationalizing the solution very simple. It becomes a matter of spinning up an appropriate dev container, installing the requirements.txt file on the container and we should have a working solution. | If your use case needs to be integrated into an existing infrastructure, then I can work with your dev-ops and ml-ops team to get this integrated. The solution can be containerized and I can work with your dev-ops team to do what it takes to configure the containerized solution to what your dev-ops team needs. For example, if you have a platform solution with specific interface constraints, I can work with that. | If the solution is a machine learning solution, I can work with your dev-ops team to set up monitoring for the solution. | . Once the solution is operationalized, then one or more of the following temporal events happen: . | There is a standard operating procedure related to the developed task, for example, you may be doing performance reporting or evaluation every quarter and at the start of the new quarter, you need to run the use case on new data. | In the case of machine learning applications, a data drift may occur. This implies that the data that the model was built or trained on is different from the data you are now receiving, so a new model based on the current data needs to be built. | . In both these cases, you revisit the modeling phase make adjustments, and then reoperationalize the new solution. This is something you can do yourself if you or your team are comfortable doing it or I can do that for you. A schematic showing this process is shown below. | Please see the development tools for details of development tools that I currently use. | . Rinse and Repeat, after iteration # 1 Footnotes . | Our discussion of the data requirements of the task is an estimated set of attributes, the exact set will be determined during implementation. &#8617; . | pandera is an example of a data quality tool. See the repository for an example of data quality rules are developed and applied to a real world dataset. &#8617; . | Of course, it is possible that your entire use case is the development of a data product. &#8617; . | . ",
    "url": "/about/#operationalizing-the-solution",
    
    "relUrl": "/about/#operationalizing-the-solution"
  },"7": {
    "doc": "Overview",
    "title": "Overview",
    "content": "Here is the anatomy of a rinse and repeat data science project. ",
    "url": "/about/",
    
    "relUrl": "/about/"
  },"8": {
    "doc": "Algorithms",
    "title": "Algorithms",
    "content": "Placeholder for rinse and repeat algorithms ",
    "url": "/recipes/algorithms/",
    
    "relUrl": "/recipes/algorithms/"
  },"9": {
    "doc": "Development",
    "title": "Development",
    "content": "Notes about topics in the design and development of data science solutions. ",
    "url": "/development/",
    
    "relUrl": "/development/"
  },"10": {
    "doc": "Analytic Recipes",
    "title": "Analytic Recipes",
    "content": "Placeholder for rinse and repeat analytic recipes ",
    "url": "/recipes/analytics/",
    
    "relUrl": "/recipes/analytics/"
  },"11": {
    "doc": "Focus Areas",
    "title": "Focus Areas",
    "content": "This site may give you a flavor of the the kind of problems I am interested in, but there is much to add and this is a work in progress. I spent a long time developing software for business tasks in various domains, I find reformulation of these problems in terms of algorithmic or learning tasks enjoyable and interesting. With that said, here is a reasonable summary of my professional interests. | Framing business problems as algorithmic problems . | Framing business problems as machine learning tasks . | Framing business problems as statistical modelling tasks . | Doing the above with a satisficing perspective if need be. Optimal is a worthy goal, but efficient learning is a worthier goal. By efficient learning, I mean learning with time and costs that are just enough to meet the business need. | Framing big data problems in terms of newer algorithmic methods such as submodularity and sketching (see this playlist ) so that large problems can be solved with sufficient (satisficing at work again) solution quality with a fraction of the costs and resources. I really believe in adding complexity only if needed, but definitely if needed. | . ",
    "url": "/interests/",
    
    "relUrl": "/interests/"
  },"12": {
    "doc": "Tools and Utilities",
    "title": "Tools and Utilities",
    "content": "As someone who has worked in software development for over two decades, the range, versatility and quality of tools we have today to facilitate every aspect of data centric application design amazes me. Today, we have very high quality open source implementations for almost every task we encounter in developing a data science solution. It is fairly accurate to say that, as a data science solution developer, more than ever before, data science solutiond developers can devote more of thier time to use case and application specific issues rather than wrangling with peripheral implementation issues that don’t really add value to the solution being developed. This page is a running list of some really impressive tools that I have across. In no particular order, these are: . | Data Cleaning: Some really awesome tools for data cleaning that I ran into recently are: . | Dtale: Good tool to quickly inspect and explore a dataset, reasonably good documentation. I used this for exploring support groups in the ITSM datasetnotebook . The github repository has many references to detailed documentation. | . | Klib: I liked the missing value feature in this library. Not very sure if it is still maintained or under active development. I used this library for exploring the ITSM dataset | . | Autoclean: I personally liked this tool the best for data cleaning. It takes a pipeline centric view of the data cleaning process. It aims at automatically identifying the issues and taking sensible default actions to fixing these issues. So ideally, you should be able to start with a raw dataset at the source end of the pipeline and receive a fully processed datafile that is ready for data analysis as a product. I tried using this to fix the raw data errors on the retail dataset. I will personally be watching this project. Kudos to the author. Great work. | . | AI-Einblick-Prompt: How can we miss generative AI today. This tool can be installed as Jupyter notebook extension/widget. You can use prompts to generate pandas code for reading files, cleaning and pre-processing tasks - drop null values etc. There is a full featured product with a drawing canvas and more UI bells and whistles. It is also fully hosted. I used the Jupyter widget version for generating the data file for the bi-clustering task for the retail example. | . | Development Containers: I evaluated github codespaces. It makes porting and operationalizing your application very straight forward. This would definitely relieve a lot of pain and friction in operationalizing a developed data science solution. This talk provides an overview of the features and the problem it aims to solve. Personally, I love this idea. | . | Exporatory Data Analysis: Dtale seemed like it did a reasonable job for basic exploration. | . | Data Quality: Pandera was pretty easy to use compared to great expectations. An example of applying this to the retail dataset is available here. | . | Stream Processing: I do think we will see the need for analytics and machine learning applications move towards stream processing. The stream algorithms in the data sketches library are pretty impressive. I tried using this on retail dataset to determine frequently purchased items. This is a Kafka based solution. A container specification for the solution is available here. | . | Big Data Processing: As datasets get bigger, finding a representative dataset for your particular problem can be an important usecase. Sketching, Sub-Modularity and Bayesian Experimental Approaches can be applied for this. Stay tuned for examples illustrating these ideas. | . ",
    "url": "/development/recipe_1.html",
    
    "relUrl": "/development/recipe_1.html"
  },"13": {
    "doc": "BiClustering",
    "title": "Algorithm Description",
    "content": "Clustering is a common data analysis technique applied to understanding the data associated with a problem. In a clustering solution, the same attributes are associated with every row of the dataset. In other words, each row is expressed or explained by all the attributes (columns). In contrast, a biclustering solution expresses associations between a subset of the rows and a subset of the columns. Examples . Biclustering usually finds application with high dimensional datasets such as those found in: . | Customer profiles, such as those found in Customer 360 use cases. Such profiles capture taste signatures that represent the interaction of the customer with the organization or business. For example, a retail apparel business may keep track of the customer’s interest in each quarter or season of the year, . | Gene Expression Data occurring in bio-informatics settings, . | Text Mining and Information Retreival . | . ",
    "url": "/recipes/algorithms/recipe_1.html#algorithm-description",
    
    "relUrl": "/recipes/algorithms/recipe_1.html#algorithm-description"
  },"14": {
    "doc": "BiClustering",
    "title": "Algorithm Input Data Description",
    "content": "A typical dataset associated with a biclustering task represents an interaction between entities. Interaction between two entities is the most common scenario. The two entities could be: . | A customer and the store inventory in a retail store customer 360 use case. The interaction between these two entities would be the amount of money a customer has spent on an inventory item. The inventory items would constitute the columns of the data matrix, and the customers to the store would represent the rows. | A document and the vocabulary of a text corpus in a text mining application. The rows represent documents and the columns represent the words in the vocabulary. If a vocabulary word occurs in a document, then the interaction has an entry (for example a count of the number of times the word occurs in the document) . | A patient and a gene expression level in a bio-informatics use case. The rows represent a patient and the columns represent the expression level of a gene in that patient’s medical test. | . ",
    "url": "/recipes/algorithms/recipe_1.html#algorithm-input-data-description",
    
    "relUrl": "/recipes/algorithms/recipe_1.html#algorithm-input-data-description"
  },"15": {
    "doc": "BiClustering",
    "title": "Algorithm Solution Description",
    "content": "The repository contains a retail dataset from the UCI machine learning repository. This dataset represents sales transaction logs from an online retail store. The sales at the store for the first quarter of 2010 are computed from this data. A row in the computed dataset is the day of the quarter, columns represent the amounts of inventory items sold that day. A biclustering of this dataset yields the first quarter sales summary for that year. A bicluster represents the days of the first quarter and the associated group of inventory item sales on those days. In the example provided, two biclusters were used. So we have two groupings of business days. Each grouping is characterized by sales activity for a subset of inventory items. See the figure below for an illustration. In other words, the biclusters can be viewed as a Q1 shopping activity summary. As with the task recipe, the implementation for the example follows the template discussed in the overview section. For the details of the algorithm and its relationship with tasks such as Singular Value Decomposition (SVD) and Principal Components Analysis (PCA), see this video . ",
    "url": "/recipes/algorithms/recipe_1.html#algorithm-solution-description",
    
    "relUrl": "/recipes/algorithms/recipe_1.html#algorithm-solution-description"
  },"16": {
    "doc": "BiClustering",
    "title": "Related Algorithms",
    "content": "Algorithms that factorize the data matrix, like PCA and SVD, are related to this algorithm. The factorization associated with this technique is different from methods such as SVD and PCA. Clustering algorithms are related to this algorithm. If your use case needs to characterize user taste, user behavior, or user activity over a time period, this algorithm may be a good candidate to evaluate. If you are looking to solve such problems, do get in touch . ",
    "url": "/recipes/algorithms/recipe_1.html#related-algorithms",
    
    "relUrl": "/recipes/algorithms/recipe_1.html#related-algorithms"
  },"17": {
    "doc": "BiClustering",
    "title": "BiClustering",
    "content": " ",
    "url": "/recipes/algorithms/recipe_1.html",
    
    "relUrl": "/recipes/algorithms/recipe_1.html"
  },"18": {
    "doc": "Event Duration",
    "title": "Description",
    "content": "This recipe computes the duration of an event of interest in time-stamped data. Such time-stamped data often arises with process data, but in general, this recipe can be applied to any machine-generated dataset with a timestamp and an event of interest. The git repository for this recipe contains an example of process data from an IT help desk. The event of interest is the resolution of the ticket. The duration is associated with the time interval between the opening of the ticket and the resolution of the ticket. Examples . | Hospital Billing Event Log : This dataset is related to events generated from a hospital billing system. A patient’s journey through the hospital can be traced. See work using this dataset, for example this paper for traces that may be of interest. The duration between two states of the trace, not necessarily the start and terminal state of the trace may be of interest to health care IT type applications wanting to baseline and monitor key performance indicators of hospital operations. | . | Sensor Data to monitor activities of daily living : Some population groups, such as professional athletes, the elderly, or the sick may have wearable sensors to monitor their activities. Blood sugar readings from a continuous glucose monitor are another example. The event of interest may be an activity like exercise or falling for seniors. For sportsmen, it may be the time taken to perform a task that is part of their training routine. For the blood sugar monitoring example, an event of interest may be a time for which the blood sugar level remains within an acceptable range. | . | Data from a manufacturing process : A manufacturing process may consist of several discrete sequential manufacturing steps. The process data is a stream of timestamped events as they are completed. An event of interest may be the time to perform a particular step. | . ",
    "url": "/recipes/analytics/recipe_1.html#description",
    
    "relUrl": "/recipes/analytics/recipe_1.html#description"
  },"19": {
    "doc": "Event Duration",
    "title": "Task Data Description",
    "content": "The key elements associated with a data description for this recipe include: . | The data attributes associated with the event duration . | The rationale to compute the event duration from these attributes. | . ",
    "url": "/recipes/analytics/recipe_1.html#task-data-description",
    
    "relUrl": "/recipes/analytics/recipe_1.html#task-data-description"
  },"20": {
    "doc": "Event Duration",
    "title": "Task Solution Description",
    "content": "See the ITSM example in the repository, for a reference implementation of this recipe. This implementation follows the template discussed in the overview section. Specifically: . | The attributes that are relevant to the use case are first defined. | Inadmissible values for the attributes are defined first and processed next. In this implementation, inadmissible values are removed (processing choice), but this can depend on the noise type and a different processing choice can be made. For example, if an attribute is missing, an imputation for the missing value can be made. | Suitable transformations are made to the dataset. In this case, a new attribute called resolution time was defined. | At the end of the transformation step the dataset is ready for report generation. The report generation step generates the required visualizations. | . See the figure below for an example of a visualization produced through this recipe. The diagram shows a histogram that captures the ticket resolution times for a particular support group in an IT help desk. For this group, the plot shows that ticket resolution times cluster into groups. Most tickets are resolved in \\(1000\\) hours or so. There are small clusters of tickets that take longer to resolve that the organization may be looking to assess and improve. There is a cluster of tickets that take between \\(1500\\) to \\(2000\\) hours, a cluster centered around \\(2500\\) hours, and a cluster between \\(3500\\) to \\(4000\\) hours. ",
    "url": "/recipes/analytics/recipe_1.html#task-solution-description",
    
    "relUrl": "/recipes/analytics/recipe_1.html#task-solution-description"
  },"21": {
    "doc": "Event Duration",
    "title": "Related Recipes",
    "content": "This recipe computes the time interval associated with an event. A related recipe could compute another metric associated with an event (Event Metric Computation). For example, an online store may be interested in weekly sales revenue. The event is the sales activity (between two consecutive Sundays, for example) during a particular business week. For each business week, sales revenue can be computed. This is the metric of interest. I have used this recipe in my research. This work was based on the retail dataset from the UCI machine learning repository. Computing the probabilistic view of event duration (Event Completion Probability) is a related recipe. This recipe would help us obtain a probabilistic estimate of the event duration. This could be useful for example to set customer expectations and for decision-theoretic models using the event. The figure below illustrates a probablistic plot for the ticket resolution data. This plot can be used to estimate the probability that a support group will resolve an assigned ticket within a specified period (number of hours). The \\(x\\) axis represents the time to resolution. The \\(y\\) axis represents the probability that the ticket would be resolved within a particular number of hours. This could be useful for organizations to set up service level agreements for support groups, the time a support group staff member may be available to work on the next assignment. In a manufacturing or healthcare setting, this recipe could be used to summarize the time to completion of a task or a phase of a treatment process. This and other recipes will be added to this site in due course. If your use case needs to analyze event durations or develop temporal event-based models, this recipe could be useful. If you are looking to solve such problems, do get in touch . ",
    "url": "/recipes/analytics/recipe_1.html#related-recipes",
    
    "relUrl": "/recipes/analytics/recipe_1.html#related-recipes"
  },"22": {
    "doc": "Event Duration",
    "title": "Event Duration",
    "content": " ",
    "url": "/recipes/analytics/recipe_1.html",
    
    "relUrl": "/recipes/analytics/recipe_1.html"
  },"23": {
    "doc": "Action Cost",
    "title": "Description",
    "content": "Frequently, analytics use cases require us to compute the cost of taking an action and then evaluating the upside and downsides to taking that action. Knowing the cost of taking an action can help us apply decision theoretic or tools from economics to make an optimal choice for a particular use case. Examples . | Fraud Detection: Should an application recommend rejection of a loan application or an approval of a loan application | . | Data Deduplication and Record Linking : In data integration applications, we need to evaluate if two documents that share information refer to the same entity. | . | Computer and Network Security : Is the new request received by a network data processing unit similar to a malicious request? | . ",
    "url": "/recipes/analytics/recipe_2.html#description",
    
    "relUrl": "/recipes/analytics/recipe_2.html#description"
  },"24": {
    "doc": "Action Cost",
    "title": "Task Data Description",
    "content": "To put this in a simple decision theoretic setting, we will consider the case where there are two actions. Let’s call them the positve and the negative action. To make a cost based decision we need the following: . | The cost of making a correct (optimal decision), this can be zero. | The cost of a false positive. If we have data, we can compute an expected value of a false positive. If we have access to a domain expert, we could ask them to provide an estimate for us. | The cost of a false negative. As with the false positive cost, this too can be estimated from data if it is available or provided to us by a domain expert. | . NOTE: A/B testing can be employed for certain problems. ",
    "url": "/recipes/analytics/recipe_2.html#task-data-description",
    
    "relUrl": "/recipes/analytics/recipe_2.html#task-data-description"
  },"25": {
    "doc": "Action Cost",
    "title": "Task Solution Description",
    "content": "See the example in the learning with costs repository, for an example of a cost calculation. This example deals with loans backed by the Small Business Administration. This example deals with 7(a) loans made to small businesses. The positive task is associated with a loan that is cancelled or charged off. The negative task is associated with a loan that is repaid in full. Machine learning applications with this data can make better decisions when the cost of a false positive is known. A false positive is incurred by a machine learning model when it predicts that a loan will be cancelled when in reality it turns out to be paid in full. A false negative is incurred by a machine learning model when it predicts that a loan will be paid in full but in reality it turns out that the loan needs to be cancelled. | To calculate the false positive costs, we estimate the expected value of the interest earned from the loan over the life of the loan using the set of loans that are paid in full. | To calculate the false negative costs, we estimate the expected value of the loan principal amounts for the set of loans that have to cancelled. | . ",
    "url": "/recipes/analytics/recipe_2.html#task-solution-description",
    
    "relUrl": "/recipes/analytics/recipe_2.html#task-solution-description"
  },"26": {
    "doc": "Action Cost",
    "title": "Related Recipes",
    "content": ". | Learning with costs . | A/B testing . | . ",
    "url": "/recipes/analytics/recipe_2.html#related-recipes",
    
    "relUrl": "/recipes/analytics/recipe_2.html#related-recipes"
  },"27": {
    "doc": "Action Cost",
    "title": "Action Cost",
    "content": " ",
    "url": "/recipes/analytics/recipe_2.html",
    
    "relUrl": "/recipes/analytics/recipe_2.html"
  },"28": {
    "doc": "Learning with Costs",
    "title": "Description",
    "content": "Business or organizational actions taken on the basis of a prediction from a machine learning model have business or organizational consequences. For example, in a Banking application, rejecting a loan applicant who turns out to be credit worthy results in lost revenue to the Bank. Approving a loan to a defaulter, could result in a bigger loss. Excessive security redirections in a business website could be a detractor to potential customers. Lax security measures could result in legal charges and financial losses. Making decisions based on costa associated with actions is a critical need. (Elkan, 2001) provides an excellent discussion of the basic concepts used in developing the theory around cost based decision making in machine learning applications. (Ling &amp; Sheng, 2008) is a more recent (still over ten years old as of this writing) treatment that surveys approaches to cost based decision making. A key characteristic of making decisions where cost is relevant using supervised learning is that such applications are usually associated with imbalanced datasets. Some classes of the target variable are under-represented in the dataset. To make this idea concrete, consider the binary classification setting. There are two classes, one of which is under-represented. This is called the minority class. By contrast, the over-represented class is called the majority class. This discussion follows the convention used in (Elkan, 2001). The minority class is associated with the \\(1\\) label and the majority class is associated with the \\(0\\) label. In a balanced dataset, the prior probablity of the \\(0\\) and the \\(1\\) class is \\(0.5\\). A key idea used in solving the imbalance issue is to construct a balanced data set out of the original imbalanced dataset using sampling. (Elkan, 2001) develops a theory that explains how we can create a balanced dataset out of an imbalanced dataset on the basis of costs associated with classifying an example with a particular target value (\\(0\\) or \\(1\\)). (Hart, 1968) uses an idea based on developing a balanced dataset from the imbalanced data set by preserving the neighborhood of each minority class instance to be consistent with the neighborhood of the point in imbalanced raw dataset. See this video link for more details on the idea. This principle is used developing the solution. This choice was made because (Elkan, 2001) provides the rationale to undersample the minority class but does not prescribe how to achieve it (fair enough). Random undersampling of the majority class can change the decision boundary that manifests in the original imbalanced dataset. Condensed nearest neighbors is attractive because it preserves the decision boundary associated with problem. It should be noted that the probabilities predicated by the model developed on the data may still be noisy. Probability caliberation can be applied to correct noise. This will be addressed in a separate algorithmic recipe shortly. ",
    "url": "/recipes/algorithms/recipe_2.html#description",
    
    "relUrl": "/recipes/algorithms/recipe_2.html#description"
  },"29": {
    "doc": "Learning with Costs",
    "title": "Algorithm Input Data Description",
    "content": "Costs associated with model decisions are inputs that are needed to apply this algorithm. These costs are developed based on Action Costs recipe. See this notebook for how false positive and false negative costs are estimated in a loan quality setting. The reduction of the imbalanced dataset to a balanced dataset using the techniques discussed in (Elkan, 2001) and (Hart, 1968) are illustrated in this notebook . ",
    "url": "/recipes/algorithms/recipe_2.html#algorithm-input-data-description",
    
    "relUrl": "/recipes/algorithms/recipe_2.html#algorithm-input-data-description"
  },"30": {
    "doc": "Learning with Costs",
    "title": "Algorithm Solution Description",
    "content": "Feature engineering can be very useful in datasets with class imbalance. This turned out to be the case for the example used to illustrate the learning with costs algorithm. The example uses real world data published by the Small Business Adminsitration (SBA). It provides details about the loan repayment history for businesses it assists. Loan defaults and cancellations do occur, though most loans are paid in full. Predicting loan defaults is an established application in the finance domain. Location is important in real estate. It turned out to be pivot for feature engineering done with this dataset. Exploratory data analysis revealed that defaults occur only a specific set of zip codes. By creating a derived attribute that captures if a zip code associated with a loan falls witin this “bad” set of zip codes, we can compute a very effective loan quality model. Using just the raw features can lead you to models with poor performance. This underscores the value of exploratory data analysis, feature engineering and due dilligence in developing data science solutions. The solutions using the approach by (Elkan, 2001) and (Hart, 1968) provide similar results. The imbalanced-learn library was used for the reference implementation. In this recipe, the condensed nearest neighbor approach is used for constructing the balanced dataset, other approaches are also possible, see imbalanced-learn for more details. ",
    "url": "/recipes/algorithms/recipe_2.html#algorithm-solution-description",
    
    "relUrl": "/recipes/algorithms/recipe_2.html#algorithm-solution-description"
  },"31": {
    "doc": "Learning with Costs",
    "title": "Bibliography",
    "content": ". | Elkan, C. (2001). The Foundations of Cost-Sensitive Learning. In B. Nebel (Ed.), Proceedings of the Seventeenth International Joint Conference on Artificial Intelligence, IJCAI 2001, Seattle, Washington, USA, August 4-10, 2001 (pp. 973–978). Morgan Kaufmann. | Ling, C. X., &amp; Sheng, V. S. (2008). Cost-sensitive learning and the class imbalance problem. Encyclopedia of Machine Learning, 2011, 231–235. | Hart, P. (1968). The condensed nearest neighbor rule (corresp.). IEEE Transactions on Information Theory, 14(3), 515–516. | . ",
    "url": "/recipes/algorithms/recipe_2.html#bibliography",
    
    "relUrl": "/recipes/algorithms/recipe_2.html#bibliography"
  },"32": {
    "doc": "Learning with Costs",
    "title": "Learning with Costs",
    "content": " ",
    "url": "/recipes/algorithms/recipe_2.html",
    
    "relUrl": "/recipes/algorithms/recipe_2.html"
  },"33": {
    "doc": "Entity Demand/Load",
    "title": "Description",
    "content": "In general, web applications capture interactions between business services and entities and human users. The frequency of such interactions serve as a measure of demand for the entity or the load on a service. Examples . In the retail domain, the demand for an entity could feed into applications such as dynamic pricing models. These models need information about the demand for an entity. The repository also contains an example from IT service management. This recipe can be applied to compute the ticket load for help desk support groups. This information can be used for planning and scheduling purposes. ",
    "url": "/recipes/analytics/recipe_3.html#description",
    
    "relUrl": "/recipes/analytics/recipe_3.html#description"
  },"34": {
    "doc": "Entity Demand/Load",
    "title": "Task Data Description",
    "content": "The data for this recipe should include: . | The timestamp associated with the interaction . | The identifier for the entity . | . ",
    "url": "/recipes/analytics/recipe_3.html#task-data-description",
    
    "relUrl": "/recipes/analytics/recipe_3.html#task-data-description"
  },"35": {
    "doc": "Entity Demand/Load",
    "title": "Task Solution Description",
    "content": "This recipe is quite straight forward and consists of the following steps: . | Aggeregate the interactions over time for each entity id . | Apply additional computation on the aggregation if needed by the use-case . | . An example of applying this recipe to compute the demand for store inventory products is available here. A review of the notebook will show that inventory item demand for a particular time period has a long tail distribution. A small handful of store inventory accounts for must of the business at the store. This could be used by store managers for inventory planning and pricing applications. An example of applying this recipe to compute the load on different support groups in a help desk application is available here. This information can be useful to understand the workload for the support groups for a particular period of time. These examples are from very different domains, but the recipe to compute the demand is similar. ",
    "url": "/recipes/analytics/recipe_3.html#task-solution-description",
    
    "relUrl": "/recipes/analytics/recipe_3.html#task-solution-description"
  },"36": {
    "doc": "Entity Demand/Load",
    "title": "Entity Demand/Load",
    "content": " ",
    "url": "/recipes/analytics/recipe_3.html",
    
    "relUrl": "/recipes/analytics/recipe_3.html"
  },"37": {
    "doc": "Frequent Items (Big Data)",
    "title": "Description",
    "content": "Online applications dealing with large data flows, for example network equipment, monitoring applications, high traffic commerce websites etc., frequently need to generate activity snapshots representing a time window and the salient entities and or events in that time window. Examples . | In an online store setting, a store operator may be interested in knowing the inventory items that are very popular and in demand during a particular time window. This information can be used for pricing and capacity planning. | A network operator may be interested in knowing the hosts that communicate very often and account for a large portion of the traffic flow in the network. | An application monitoring IOT sensor/device events in a hospital network may want to summarize the major events that occured in a particular week. | . All of these applications are characterized by high volumes of data flow. These applications may also need to be deployed on resource constrained hardware settings. Further, because of the data volume and the magnitude of the metrics under consideration, approximate answers are often sufficient. ",
    "url": "/recipes/analytics/recipe_4.html#description",
    
    "relUrl": "/recipes/analytics/recipe_4.html#description"
  },"38": {
    "doc": "Frequent Items (Big Data)",
    "title": "Task Data Description",
    "content": "The data for this recipe is a stream. A stream abstraction is a sequence of tuples where each tuple includes: . | The timestamp associated with the interaction . | The identifier for the entity . | The metric of interest, for example, the traffic flow rate, the damand in units, magnitude of measurement etc. | . ",
    "url": "/recipes/analytics/recipe_4.html#task-data-description",
    
    "relUrl": "/recipes/analytics/recipe_4.html#task-data-description"
  },"39": {
    "doc": "Frequent Items (Big Data)",
    "title": "Task Solution Description",
    "content": "Frequent Items is one of the most important and most commonly applied big data algorithms. See this workshop series for more information. This recipe is implemented using the popular data sketches library. The recipe is implemented using a dockerized single node confluent kafka implementation. The solution consists of: . | An admin client to set up the topic for the example . | A producer component that streams the daily demand for store inventory items to the topic . | A consumer client that reads the stream and uses the frequent items algorithm to estimate the top 10 most frequent inventory items sold at the store for a particular time period (first quarter of the year 2010.) . | . The details the implementation are available in this repository. More big data recipes will be added shortly. ",
    "url": "/recipes/analytics/recipe_4.html#task-solution-description",
    
    "relUrl": "/recipes/analytics/recipe_4.html#task-solution-description"
  },"40": {
    "doc": "Frequent Items (Big Data)",
    "title": "Frequent Items (Big Data)",
    "content": " ",
    "url": "/recipes/analytics/recipe_4.html",
    
    "relUrl": "/recipes/analytics/recipe_4.html"
  }
}
